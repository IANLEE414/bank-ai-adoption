{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f4322-63f9-46ca-9af5-fb39f32b968e",
   "metadata": {
    "id": "3e4f4322-63f9-46ca-9af5-fb39f32b968e",
    "outputId": "b674d6ba-c439-46b8-e765-42ca6f2c58e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from python-dateutil) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dltjr\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas lxml openpyxl python-dateutil tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9af44-56a6-43ad-ad25-9b5be938c882",
   "metadata": {
    "id": "3ac9af44-56a6-43ad-ad25-9b5be938c882",
    "outputId": "09080683-15e1-4f6b-f75d-7169415e5434"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Check outputs/ folder.\n"
     ]
    }
   ],
   "source": [
    "# ai_adoption_crawler.py\n",
    "# 목적: 은행별 공식/준공식 보도자료에서 AI(인공지능) + (신용평가/대출심사/자동심사/FDS/이상거래) 키워드를 포함한 문서를 수집\n",
    "#      -> 근거 URL/제목/날짜/스니펫을 저장하고, 은행별 최초 연도를 후보로 추출\n",
    "#\n",
    "# 출력:\n",
    "#   outputs/raw_hits.csv\n",
    "#   outputs/raw_hits.xlsx\n",
    "#   outputs/adoption_year_candidates.csv\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# 0) 공통 설정\n",
    "# ----------------------------\n",
    "\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "AI_KEYWORDS = [\"AI\", \"인공지능\", \"Artificial Intelligence\"]\n",
    "CREDIT_KEYWORDS = [\"신용평가\", \"대출심사\", \"자동심사\", \"CSS\", \"FDS\", \"이상거래\", \"AML\", \"리스크\", \"credit scoring\", \"underwriting\"]\n",
    "\n",
    "# 키워드 동시충족 조건:\n",
    "#  - AI키워드 >=1 AND (대출/신용/FDS 계열 키워드)>=1\n",
    "def keyword_hit(text: str) -> Tuple[bool, List[str], List[str]]:\n",
    "    t = text or \"\"\n",
    "    ai_hits = [k for k in AI_KEYWORDS if k.lower() in t.lower()]\n",
    "    cr_hits = [k for k in CREDIT_KEYWORDS if k.lower() in t.lower()]\n",
    "    return (len(ai_hits) > 0 and len(cr_hits) > 0), ai_hits, cr_hits\n",
    "\n",
    "def normalize_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def safe_filename(url: str) -> str:\n",
    "    h = hashlib.md5(url.encode(\"utf-8\")).hexdigest()[:12]\n",
    "    return h\n",
    "\n",
    "def parse_date_any(s: str) -> Optional[str]:\n",
    "    \"\"\"다양한 날짜표현에서 YYYY-MM-DD로 정규화\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    # 흔한 형식: 2026.02.13 / 2026-02-13 / 2026/02/13\n",
    "    m = re.search(r\"(20\\d{2})[.\\-/](\\d{1,2})[.\\-/](\\d{1,2})\", s)\n",
    "    if m:\n",
    "        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))\n",
    "        return f\"{y:04d}-{mo:02d}-{d:02d}\"\n",
    "    # 파싱 시도\n",
    "    try:\n",
    "        dt = dateparser.parse(s, fuzzy=True)\n",
    "        if dt:\n",
    "            return dt.date().isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def extract_year(date_iso: Optional[str]) -> Optional[int]:\n",
    "    if not date_iso:\n",
    "        return None\n",
    "    try:\n",
    "        return int(date_iso[:4])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def log(msg: str):\n",
    "    path = os.path.join(OUTPUT_DIR, \"run_log.txt\")\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg + \"\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) 크롤러 베이스\n",
    "# ----------------------------\n",
    "\n",
    "@dataclass\n",
    "class Article:\n",
    "    bank: str\n",
    "    url: str\n",
    "    title: str\n",
    "    date: Optional[str]   # ISO\n",
    "    text: str\n",
    "    ai_hits: List[str]\n",
    "    credit_hits: List[str]\n",
    "    snippet: str\n",
    "\n",
    "class BaseCrawler:\n",
    "    bank: str\n",
    "\n",
    "    def __init__(self, bank: str, session: Optional[requests.Session] = None):\n",
    "        self.bank = bank\n",
    "        self.sess = session or requests.Session()\n",
    "        self.sess.headers.update(HEADERS)\n",
    "\n",
    "    def list_article_urls(self) -> List[Tuple[str, Optional[str], Optional[str]]]:\n",
    "        \"\"\"\n",
    "        returns list of tuples: (url, title_hint, date_hint)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fetch_article(self, url: str, title_hint: Optional[str] = None, date_hint: Optional[str] = None) -> Article:\n",
    "        \"\"\"\n",
    "        기본: HTML -> text 추출.\n",
    "        사이트별로 override 가능.\n",
    "        \"\"\"\n",
    "        r = self.sess.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "        # title 추정\n",
    "        title = title_hint or \"\"\n",
    "        if not title:\n",
    "            og = soup.select_one('meta[property=\"og:title\"]')\n",
    "            if og and og.get(\"content\"):\n",
    "                title = og.get(\"content\").strip()\n",
    "        if not title:\n",
    "            if soup.title:\n",
    "                title = soup.title.get_text(strip=True)\n",
    "\n",
    "        # date 추정\n",
    "        date_iso = parse_date_any(date_hint) if date_hint else None\n",
    "        if not date_iso:\n",
    "            # 흔한 패턴들\n",
    "            # - meta property=\"article:published_time\"\n",
    "            meta_dt = soup.select_one('meta[property=\"article:published_time\"]')\n",
    "            if meta_dt and meta_dt.get(\"content\"):\n",
    "                date_iso = parse_date_any(meta_dt.get(\"content\"))\n",
    "        if not date_iso:\n",
    "            # 페이지 text에서 날짜 패턴 탐색\n",
    "            text_all = soup.get_text(\" \", strip=True)\n",
    "            m = re.search(r\"(20\\d{2})[.\\-/](\\d{1,2})[.\\-/](\\d{1,2})\", text_all)\n",
    "            if m:\n",
    "                date_iso = f\"{int(m.group(1)):04d}-{int(m.group(2)):02d}-{int(m.group(3)):02d}\"\n",
    "\n",
    "        # 본문 text\n",
    "        text = soup.get_text(\"\\n\", strip=True)\n",
    "        text = normalize_space(text)\n",
    "\n",
    "        hit, ai_hits, cr_hits = keyword_hit(text)\n",
    "\n",
    "        # 스니펫: 첫 매칭 위치 주변 250자\n",
    "        snippet = \"\"\n",
    "        if hit:\n",
    "            # 가장 먼저 등장하는 키워드 위치 찾기\n",
    "            idxs = []\n",
    "            for k in (ai_hits + cr_hits):\n",
    "                i = text.lower().find(k.lower())\n",
    "                if i >= 0:\n",
    "                    idxs.append(i)\n",
    "            if idxs:\n",
    "                i0 = max(min(idxs) - 120, 0)\n",
    "                i1 = min(min(idxs) + 250, len(text))\n",
    "                snippet = text[i0:i1]\n",
    "\n",
    "        return Article(\n",
    "            bank=self.bank,\n",
    "            url=url,\n",
    "            title=title or \"\",\n",
    "            date=date_iso,\n",
    "            text=text,\n",
    "            ai_hits=ai_hits,\n",
    "            credit_hits=cr_hits,\n",
    "            snippet=snippet\n",
    "        )\n",
    "\n",
    "# ----------------------------\n",
    "# 2) 은행별 크롤러들\n",
    "# ----------------------------\n",
    "\n",
    "class KBCrawler(BaseCrawler):\n",
    "    \"\"\"\n",
    "    KB국민은행 보도자료 (omoney.kbstar.com/quics?page=C017648) 기반\n",
    "    \"\"\"\n",
    "    BASE = \"https://omoney.kbstar.com/quics?page=C017648\"\n",
    "\n",
    "    def list_article_urls(self) -> List[Tuple[str, Optional[str], Optional[str]]]:\n",
    "        urls = []\n",
    "        # viewPage=1,2,... 페이지가 있을 수 있어 범위를 넉넉히 탐색\n",
    "        for view_page in range(1, 60):  # 필요시 증가\n",
    "            list_url = f\"{self.BASE}&bbsMode=list&boardId=647&viewPage={view_page}\"\n",
    "            r = self.sess.get(list_url, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "            rows = soup.select(\"table tr\")\n",
    "            if not rows:\n",
    "                break\n",
    "\n",
    "            found_any = False\n",
    "            for tr in rows:\n",
    "                a = tr.find(\"a\")\n",
    "                tds = tr.find_all(\"td\")\n",
    "                if a and a.get(\"href\") and len(tds) >= 3:\n",
    "                    href = a.get(\"href\")\n",
    "                    title = a.get_text(strip=True)\n",
    "                    date_hint = tds[-2].get_text(strip=True)  # 등록일 위치가 보통 뒤쪽\n",
    "                    full = urljoin(\"https://omoney.kbstar.com\", href)\n",
    "                    urls.append((full, title, date_hint))\n",
    "                    found_any = True\n",
    "            if not found_any:\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "        return urls\n",
    "\n",
    "class ShinhanGroupCrawler(BaseCrawler):\n",
    "    \"\"\"\n",
    "    신한금융그룹 그룹사 뉴스(신한은행 포함)\n",
    "    https://www.shinhangroup.com/kr/archive/press\n",
    "    \"\"\"\n",
    "    BASE = \"https://www.shinhangroup.com/kr/archive/press\"\n",
    "\n",
    "    def list_article_urls(self) -> List[Tuple[str, Optional[str], Optional[str]]]:\n",
    "        urls = []\n",
    "        # 페이지 구조가 변할 수 있어: 일단 여러 페이지를 내려가며 a태그 추출\n",
    "        for page in range(1, 80):\n",
    "            list_url = f\"{self.BASE}?page={page}\"\n",
    "            r = self.sess.get(list_url, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "            # 카드형 목록에서 링크 추출\n",
    "            items = soup.select(\"a\")\n",
    "            got = 0\n",
    "            for a in items:\n",
    "                href = a.get(\"href\") or \"\"\n",
    "                txt = a.get_text(\" \", strip=True)\n",
    "                if \"[신한은행]\" in txt and href:\n",
    "                    full = urljoin(self.BASE, href)\n",
    "                    # 날짜 힌트: 주변 텍스트에서 찾기\n",
    "                    date_hint = None\n",
    "                    # 같은 카드 내 날짜를 찾기 시도\n",
    "                    parent = a.parent\n",
    "                    if parent:\n",
    "                        ptxt = parent.get_text(\" \", strip=True)\n",
    "                        date_hint = parse_date_any(ptxt)\n",
    "                    urls.append((full, txt, date_hint))\n",
    "                    got += 1\n",
    "            if got == 0:\n",
    "                # 더 이상 없으면 중단\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "        return urls\n",
    "\n",
    "class WooriCrawler(BaseCrawler):\n",
    "    \"\"\"\n",
    "    우리은행 보도자료 (spot.wooribank.com)\n",
    "    https://spot.wooribank.com/pot/Dream?withyou=BPPBC0036\n",
    "    \"\"\"\n",
    "    BASE = \"https://spot.wooribank.com/pot/Dream?withyou=BPPBC0036\"\n",
    "\n",
    "    def list_article_urls(self) -> List[Tuple[str, Optional[str], Optional[str]]]:\n",
    "        urls = []\n",
    "        for page in range(1, 80):\n",
    "            list_url = f\"{self.BASE}&page={page}\"\n",
    "            r = self.sess.get(list_url, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "            # 보도자료 목록에서 a 태그\n",
    "            anchors = soup.select(\"a\")\n",
    "            got = 0\n",
    "            for a in anchors:\n",
    "                href = a.get(\"href\") or \"\"\n",
    "                title = a.get_text(\" \", strip=True)\n",
    "                if \"Dream?withyou=\" in href and \"BPPBC\" in href and title:\n",
    "                    full = urljoin(\"https://spot.wooribank.com\", href)\n",
    "                    # 날짜 힌트: title 옆 괄호( 2026.01.15 ) 같은 게 있을 때\n",
    "                    date_hint = None\n",
    "                    m = re.search(r\"(20\\d{2}[.\\-/]\\d{1,2}[.\\-/]\\d{1,2})\", title)\n",
    "                    if m:\n",
    "                        date_hint = m.group(1)\n",
    "                    urls.append((full, title, date_hint))\n",
    "                    got += 1\n",
    "            if got == 0:\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "        return urls\n",
    "\n",
    "class NHBankCrawler(BaseCrawler):\n",
    "    \"\"\"\n",
    "    NH농협은행 NH뉴스\n",
    "    https://www.nhbank.com/goSubPage.do?srchGb=KO_NHPR_02&srchSiteGb=KR\n",
    "    \"\"\"\n",
    "    BASE = \"https://www.nhbank.com/goSubPage.do?srchGb=KO_NHPR_02&srchSiteGb=KR\"\n",
    "\n",
    "    def list_article_urls(self) -> List[Tuple[str, Optional[str], Optional[str]]]:\n",
    "        urls = []\n",
    "        for page in range(1, 120):\n",
    "            list_url = f\"{self.BASE}&srchP={page}\"\n",
    "            r = self.sess.get(list_url, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "            anchors = soup.select(\"a\")\n",
    "            got = 0\n",
    "            for a in anchors:\n",
    "                href = a.get(\"href\") or \"\"\n",
    "                title = a.get_text(\" \", strip=True)\n",
    "                if (\"goSubPage.do\" in href or \"goSubPage\" in href) and title:\n",
    "                    full = urljoin(\"https://www.nhbank.com\", href)\n",
    "                    # 날짜는 카드 텍스트에서 찾기\n",
    "                    parent = a.parent\n",
    "                    date_hint = None\n",
    "                    if parent:\n",
    "                        date_hint = parse_date_any(parent.get_text(\" \", strip=True))\n",
    "                    urls.append((full, title, date_hint))\n",
    "                    got += 1\n",
    "            if got == 0:\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "        return urls\n",
    "\n",
    "class HanaFNCrawler(BaseCrawler):\n",
    "    \"\"\"\n",
    "    하나금융 PR센터(그룹)에서 #하나은행 태그 뉴스만 추출\n",
    "    https://www.hanafn.com/mediaRoom/mediaRoom.do\n",
    "    \"\"\"\n",
    "    BASE = \"https://www.hanafn.com/mediaRoom/mediaRoom.do\"\n",
    "\n",
    "    def list_article_urls(self) -> List[Tuple[str, Optional[str], Optional[str]]]:\n",
    "        urls = []\n",
    "        for page in range(1, 100):\n",
    "            list_url = f\"{self.BASE}?pageIndex={page}\"\n",
    "            r = self.sess.get(list_url, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "            got = 0\n",
    "            # 카드 텍스트에 \"#하나은행\" 포함된 항목 찾기\n",
    "            for a in soup.select(\"a\"):\n",
    "                href = a.get(\"href\") or \"\"\n",
    "                txt = a.get_text(\" \", strip=True)\n",
    "                if \"하나은행\" in txt and href:\n",
    "                    full = urljoin(\"https://www.hanafn.com\", href)\n",
    "                    # 날짜 힌트: 근처에 2026.02.02 등\n",
    "                    date_hint = None\n",
    "                    parent = a.parent\n",
    "                    if parent:\n",
    "                        date_hint = parse_date_any(parent.get_text(\" \", strip=True))\n",
    "                    urls.append((full, txt, date_hint))\n",
    "                    got += 1\n",
    "\n",
    "            if got == 0:\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "        return urls\n",
    "\n",
    "class KakaoBankCrawler(BaseCrawler):\n",
    "    \"\"\"\n",
    "    카카오뱅크 보도자료\n",
    "    https://www.kakaobank.com/Corp/News/PressRelease/pages/1\n",
    "    \"\"\"\n",
    "    BASE = \"https://www.kakaobank.com/Corp/News/PressRelease/pages/\"\n",
    "\n",
    "    def list_article_urls(self) -> List[Tuple[str, Optional[str], Optional[str]]]:\n",
    "        urls = []\n",
    "        for page in range(1, 200):\n",
    "            list_url = f\"{self.BASE}{page}\"\n",
    "            r = self.sess.get(list_url, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "            got = 0\n",
    "            for a in soup.select(\"a\"):\n",
    "                href = a.get(\"href\") or \"\"\n",
    "                title = a.get_text(\" \", strip=True)\n",
    "                if \"/Corp/News/PressRelease/\" in href and href.endswith(tuple(\"0123456789\")):\n",
    "                    full = urljoin(\"https://www.kakaobank.com\", href)\n",
    "                    # 날짜 힌트: 카드 내에 2023.04.11 같은 표기가 있음\n",
    "                    date_hint = None\n",
    "                    parent = a.parent\n",
    "                    if parent:\n",
    "                        date_hint = parse_date_any(parent.get_text(\" \", strip=True))\n",
    "                    urls.append((full, title, date_hint))\n",
    "                    got += 1\n",
    "            if got == 0:\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "        return urls\n",
    "\n",
    "class TossbankWriterCrawler(BaseCrawler):\n",
    "    \"\"\"\n",
    "    토스피드 writer/tossbank (토스뱅크 공식 writer 페이지)\n",
    "    https://toss.im/tossfeed/writer/tossbank\n",
    "    \"\"\"\n",
    "    BASE = \"https://toss.im/tossfeed/writer/tossbank\"\n",
    "\n",
    "    def list_article_urls(self) -> List[Tuple[str, Optional[str], Optional[str]]]:\n",
    "        urls = []\n",
    "        # 페이지네이션이 SPA 형태일 수 있어, 우선 단순 페이지 파라미터 시도\n",
    "        # 안 될 경우(기사 일부만 수집) selenium 확장 필요\n",
    "        for page in range(1, 40):\n",
    "            list_url = f\"{self.BASE}?page={page}\"\n",
    "            r = self.sess.get(list_url, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "            got = 0\n",
    "            for a in soup.select(\"a\"):\n",
    "                href = a.get(\"href\") or \"\"\n",
    "                title = a.get_text(\" \", strip=True)\n",
    "                if \"/tossfeed/article/\" in href and title:\n",
    "                    full = urljoin(\"https://toss.im\", href)\n",
    "                    # 날짜 힌트\n",
    "                    date_hint = None\n",
    "                    parent = a.parent\n",
    "                    if parent:\n",
    "                        date_hint = parse_date_any(parent.get_text(\" \", strip=True))\n",
    "                    urls.append((full, title, date_hint))\n",
    "                    got += 1\n",
    "            if got == 0:\n",
    "                # 더 없으면 종료\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "        return urls\n",
    "\n",
    "class FSCPressCrawler(BaseCrawler):\n",
    "    \"\"\"\n",
    "    금융위원회 보도자료 검색 기반 (케이뱅크 등)\n",
    "    예: https://www.fsc.go.kr/no010101/81348?... (개별 문서)\n",
    "    검색 목록은 no010101?srchText=...\n",
    "    \"\"\"\n",
    "    SEARCH_BASE = \"https://www.fsc.go.kr/no010101\"\n",
    "\n",
    "    def __init__(self, bank: str, query: str, session: Optional[requests.Session] = None):\n",
    "        super().__init__(bank, session=session)\n",
    "        self.query = query\n",
    "\n",
    "    def list_article_urls(self) -> List[Tuple[str, Optional[str], Optional[str]]]:\n",
    "        urls = []\n",
    "        for page in range(1, 80):\n",
    "            list_url = f\"{self.SEARCH_BASE}?curPage={page}&srchKey=all&srchText={requests.utils.quote(self.query)}\"\n",
    "            r = self.sess.get(list_url, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "            got = 0\n",
    "            for a in soup.select(\"a\"):\n",
    "                href = a.get(\"href\") or \"\"\n",
    "                title = a.get_text(\" \", strip=True)\n",
    "                # 보도자료 상세는 보통 /no010101/숫자 형태\n",
    "                if re.search(r\"/no010101/\\d+\", href) and title:\n",
    "                    full = urljoin(\"https://www.fsc.go.kr\", href)\n",
    "                    # 날짜 힌트: 목록 주변 텍스트에서 찾기\n",
    "                    date_hint = None\n",
    "                    parent = a.parent\n",
    "                    if parent:\n",
    "                        date_hint = parse_date_any(parent.get_text(\" \", strip=True))\n",
    "                    urls.append((full, title, date_hint))\n",
    "                    got += 1\n",
    "            if got == 0:\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "        return urls\n",
    "\n",
    "# ----------------------------\n",
    "# 3) 실행 함수\n",
    "# ----------------------------\n",
    "\n",
    "def run_all(limit_each: Optional[int] = None) -> pd.DataFrame:\n",
    "    session = requests.Session()\n",
    "    session.headers.update(HEADERS)\n",
    "\n",
    "    crawlers: List[BaseCrawler] = [\n",
    "        KBCrawler(\"국민은행\", session=session),\n",
    "        ShinhanGroupCrawler(\"신한은행\", session=session),\n",
    "        WooriCrawler(\"우리은행\", session=session),\n",
    "        NHBankCrawler(\"농협은행\", session=session),\n",
    "        HanaFNCrawler(\"하나은행\", session=session),\n",
    "        KakaoBankCrawler(\"카카오뱅크\", session=session),\n",
    "        TossbankWriterCrawler(\"토스뱅크\", session=session),\n",
    "\n",
    "        # 케이뱅크: 은행 자체 보도자료 아카이브가 공개형으로 명확히 확인되지 않아\n",
    "        # \"정부(금융위원회) 보도자료\"에서 '케이뱅크'를 검색하여 공식 근거 확보\n",
    "        FSCPressCrawler(\"케이뱅크\", query=\"케이뱅크\", session=session),\n",
    "\n",
    "        # 기업은행: 공식 홈페이지 PR 아카이브가 크롤링 난이도/차단 이슈가 있어,\n",
    "        # 연구 재현성을 위해 우선 보도자료 배포 플랫폼(뉴스와이어)도 함께 쓰는 방식 권장.\n",
    "        # (원하면 IBK 전용 selenium 버전 추가 제공 가능)\n",
    "        # 여기서는 예시로 금융위 검색도 함께 추가(공식 문서 기준)\n",
    "        FSCPressCrawler(\"기업은행\", query=\"기업은행\", session=session),\n",
    "    ]\n",
    "\n",
    "    hits: List[Article] = []\n",
    "    log(f\"[RUN] start at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    for crawler in crawlers:\n",
    "        log(f\"[CRAWLER] {crawler.bank} listing...\")\n",
    "        try:\n",
    "            items = crawler.list_article_urls()\n",
    "        except Exception as e:\n",
    "            log(f\"[ERROR] list failed: {crawler.bank} :: {e}\")\n",
    "            continue\n",
    "\n",
    "        if limit_each:\n",
    "            items = items[:limit_each]\n",
    "\n",
    "        log(f\"[CRAWLER] {crawler.bank} listed {len(items)} urls\")\n",
    "\n",
    "        for (url, title_hint, date_hint) in tqdm(items, desc=f\"Fetch {crawler.bank}\", leave=False):\n",
    "            try:\n",
    "                art = crawler.fetch_article(url, title_hint=title_hint, date_hint=date_hint)\n",
    "                ok, _, _ = keyword_hit(art.text)\n",
    "                if ok:\n",
    "                    hits.append(art)\n",
    "\n",
    "                    # 원문 증거를 남기기 위해 html/text 일부도 저장(텍스트만)\n",
    "                    fn = safe_filename(url)\n",
    "                    with open(os.path.join(OUTPUT_DIR, f\"{crawler.bank}_{fn}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(art.text)\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"[ERROR] fetch failed: {crawler.bank} :: {url} :: {e}\")\n",
    "                continue\n",
    "\n",
    "    log(f\"[RUN] done at {time.strftime('%Y-%m-%d %H:%M:%S')}, hits={len(hits)}\")\n",
    "\n",
    "    # dataframe 변환\n",
    "    df = pd.DataFrame([{\n",
    "        \"bank\": a.bank,\n",
    "        \"date\": a.date,\n",
    "        \"year\": extract_year(a.date),\n",
    "        \"title\": a.title,\n",
    "        \"url\": a.url,\n",
    "        \"ai_hits\": \", \".join(a.ai_hits),\n",
    "        \"credit_hits\": \", \".join(a.credit_hits),\n",
    "        \"snippet\": a.snippet\n",
    "    } for a in hits]).sort_values([\"bank\", \"date\", \"title\"], na_position=\"last\")\n",
    "\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, \"raw_hits.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    df.to_excel(os.path.join(OUTPUT_DIR, \"raw_hits.xlsx\"), index=False)\n",
    "\n",
    "    # 은행별 후보(최초 연도)\n",
    "    cand = (\n",
    "        df.dropna(subset=[\"year\"])\n",
    "          .groupby(\"bank\")[\"year\"]\n",
    "          .min()\n",
    "          .reset_index()\n",
    "          .rename(columns={\"year\": \"AI_adopt_year_candidate\"})\n",
    "          .sort_values(\"bank\")\n",
    "    )\n",
    "    cand.to_csv(os.path.join(OUTPUT_DIR, \"adoption_year_candidates.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # limit_each를 None으로 두면 가능한 범위를 최대 탐색\n",
    "    # 처음 디버깅은 limit_each=50 같은 식으로 추천\n",
    "    run_all(limit_each=None)\n",
    "    print(\"Done. Check outputs/ folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89c397-69fa-4db2-aef5-d6746913c35c",
   "metadata": {
    "id": "dc89c397-69fa-4db2-aef5-d6746913c35c"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) 로드\n",
    "df = pd.read_csv(\"raw_hits.csv\")\n",
    "\n",
    "# 2) 공백/NA 정리\n",
    "for c in [\"bank\",\"date\",\"year\",\"title\",\"url\",\"ai_hits\",\"credit_hits\",\"snippet\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(str).replace({\"nan\": \"\", \"None\": \"\"}).str.strip()\n",
    "\n",
    "# 3) URL 없는 행 제거 (핵심키)\n",
    "df = df[df[\"url\"].ne(\"\")].copy()\n",
    "\n",
    "# 4) date 파싱 + year 재생성(우선 date가 멀쩡한 것만)\n",
    "df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"year_from_date\"] = df[\"date_parsed\"].dt.year\n",
    "\n",
    "# 기존 year가 비었거나 이상하면 date 기반으로 교체\n",
    "df[\"year_num\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "bad_year = df[\"year_num\"].isna() | (df[\"year_num\"] < 1990) | (df[\"year_num\"] > 2030)\n",
    "df.loc[bad_year & df[\"year_from_date\"].notna(), \"year_num\"] = df.loc[bad_year & df[\"year_from_date\"].notna(), \"year_from_date\"]\n",
    "\n",
    "# 5) hit 컬럼 정규화: \"AI, 인공지능\" 같은 텍스트를 키워드 플래그로\n",
    "def norm_text(x):\n",
    "    x = (x or \"\").strip()\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    return x\n",
    "\n",
    "df[\"ai_hits_norm\"] = df[\"ai_hits\"].map(norm_text)\n",
    "df[\"credit_hits_norm\"] = df[\"credit_hits\"].map(norm_text)\n",
    "\n",
    "AI_KW = [\"AI\", \"인공지능\", \"머신러닝\", \"딥러닝\", \"생성형\", \"Gen AI\", \"챗봇\"]\n",
    "CREDIT_KW = [\"신용평가\", \"대출심사\", \"리스크\", \"AML\", \"FDS\", \"이상거래\", \"자금세탁\"]\n",
    "\n",
    "def has_kw(text, kws):\n",
    "    t = (text or \"\")\n",
    "    return any(k.lower() in t.lower() for k in kws)\n",
    "\n",
    "df[\"has_ai\"] = df[\"ai_hits_norm\"].apply(lambda t: has_kw(t, AI_KW))\n",
    "df[\"has_credit_related\"] = df[\"credit_hits_norm\"].apply(lambda t: has_kw(t, CREDIT_KW))\n",
    "\n",
    "# 6) snippet 정리: 공통 메뉴/푸터 제거(농협 경영공시 같은 케이스)\n",
    "BOILER = [\n",
    "    \"찾아오시는 길\", \"영업점 안내\", \"윤리경영\", \"금융소비자보호\", \"민원\", \"FAQ\", \"새창열림\",\n",
    "    \"CI 다운로드\", \"채용정보\", \"인사제도\", \"공시정보\"\n",
    "]\n",
    "def clean_snippet(s):\n",
    "    s = (s or \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # 보일러플레이트가 과다 포함되면 빈값 처리\n",
    "    hit = sum(1 for b in BOILER if b in s)\n",
    "    if hit >= 3:\n",
    "        return \"\"\n",
    "    # 너무 길면 잘라두기\n",
    "    return s[:400]\n",
    "\n",
    "df[\"snippet_clean\"] = df[\"snippet\"].apply(clean_snippet)\n",
    "\n",
    "# 7) 중복 제거: url 기준 (가장 강력)\n",
    "df = df.sort_values([\"url\",\"date_parsed\"], ascending=[True, False])\n",
    "df = df.drop_duplicates(subset=[\"url\"], keep=\"first\").copy()\n",
    "\n",
    "# 8) 농협 경영공시 같이 title이 너무 일반적인 것 필터(선택)\n",
    "generic_title = df[\"title\"].isin([\"경영공시\"])\n",
    "df.loc[generic_title & df[\"snippet_clean\"].eq(\"\"), \"drop_candidate\"] = True\n",
    "df[\"drop_candidate\"] = df[\"drop_candidate\"].fillna(False)\n",
    "\n",
    "# 저장\n",
    "df.to_csv(\"clean_step1.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3754961-3392-45cb-95ce-d2e4468b403e",
   "metadata": {
    "id": "b3754961-3392-45cb-95ce-d2e4468b403e",
    "outputId": "50921cc0-f244-4c18-a3ca-e93b06c30595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\ds\\anaconda3\\lib\\site-packages (0.11.9)\n",
      "Requirement already satisfied: pandas in c:\\users\\ds\\anaconda3\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ds\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pdfminer.six==20251230 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pdfplumber) (20251230)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pdfplumber) (12.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pdfplumber) (5.4.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pdfminer.six==20251230->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ds\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ds\\anaconda3\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6ce8e-4c4e-4e59-83ba-70f9e93392ae",
   "metadata": {
    "id": "cbf6ce8e-4c4e-4e59-83ba-70f9e93392ae",
    "outputId": "0fd61493-377e-40d8-a070-b54a8e404e6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/84] 읽는 중: 국민은행_사업보고서2019.pdf\n",
      "[2/84] 읽는 중: 국민은행_사업보고서2020.pdf\n",
      "[3/84] 읽는 중: 국민은행_사업보고서2021.pdf\n",
      "[4/84] 읽는 중: 국민은행_사업보고서2022.pdf\n",
      "[5/84] 읽는 중: 국민은행_사업보고서2023.pdf\n",
      "[6/84] 읽는 중: 국민은행_사업보고서2024.pdf\n",
      "[7/84] 읽는 중: 국민은행_지속가능경영2019.pdf\n",
      "[8/84] 읽는 중: 국민은행_지속가능경영2020.pdf\n",
      "[9/84] 읽는 중: 국민은행_지속가능경영2021.pdf\n",
      "[10/84] 읽는 중: 국민은행_지속가능경영2022.pdf\n",
      "[11/84] 읽는 중: 국민은행_지속가능경영2023.pdf\n",
      "[12/84] 읽는 중: 국민은행_지속가능경영2024.pdf\n",
      "[13/84] 읽는 중: 기업은행_사업보고서2019.pdf\n",
      "[14/84] 읽는 중: 기업은행_사업보고서2020.pdf\n",
      "[15/84] 읽는 중: 기업은행_사업보고서2021.pdf\n",
      "[16/84] 읽는 중: 기업은행_사업보고서2022.pdf\n",
      "[17/84] 읽는 중: 기업은행_사업보고서2023.pdf\n",
      "[18/84] 읽는 중: 기업은행_사업보고서2024.pdf\n",
      "[19/84] 읽는 중: 기업은행_지속가능경영2020.pdf\n",
      "[20/84] 읽는 중: 기업은행_지속가능경영2021.pdf\n",
      "[21/84] 읽는 중: 기업은행_지속가능경영2022.pdf\n",
      "[22/84] 읽는 중: 기업은행_지속가능경영2023.pdf\n",
      "[23/84] 읽는 중: 기업은행_지속가능경영2024.pdf\n",
      "[24/84] 읽는 중: 농협금융_지속가능경영2021.pdf\n",
      "[25/84] 읽는 중: 농협금융_지속가능경영2022.pdf\n",
      "[26/84] 읽는 중: 농협은행_감사보고서2019.pdf\n",
      "[27/84] 읽는 중: 농협은행_감사보고서2020.pdf\n",
      "[28/84] 읽는 중: 농협은행_감사보고서2021.pdf\n",
      "[29/84] 읽는 중: 농협은행_감사보고서2022.pdf\n",
      "[30/84] 읽는 중: 농협은행_감사보고서2023.pdf\n",
      "[31/84] 읽는 중: 농협은행_감사보고서2024.pdf\n",
      "[32/84] 읽는 중: 농협은행_지속가능경영2023.pdf\n",
      "[33/84] 읽는 중: 농협은행_지속가능경영2024.pdf\n",
      "[34/84] 읽는 중: 신한은행_사업보고서2019.pdf\n",
      "[35/84] 읽는 중: 신한은행_사업보고서2020.pdf\n",
      "[36/84] 읽는 중: 신한은행_사업보고서2021.pdf\n",
      "[37/84] 읽는 중: 신한은행_사업보고서2022.pdf\n",
      "[38/84] 읽는 중: 신한은행_사업보고서2023.pdf\n",
      "[39/84] 읽는 중: 신한은행_사업보고서2024.pdf\n",
      "[40/84] 읽는 중: 신한은행_지속가능경영2020.pdf\n",
      "[41/84] 읽는 중: 신한은행_지속가능경영2021.pdf\n",
      "[42/84] 읽는 중: 신한은행_지속가능경영2023.pdf\n",
      "[43/84] 읽는 중: 신한은행_지속가능경영2024.pdf\n",
      "[44/84] 읽는 중: 우리금융_지속가능경영2019.pdf\n",
      "[45/84] 읽는 중: 우리금융_지속가능경영2020.pdf\n",
      "[46/84] 읽는 중: 우리금융_지속가능경영2021.pdf\n",
      "[47/84] 읽는 중: 우리은행_사업보고서2019.pdf\n",
      "[48/84] 읽는 중: 우리은행_사업보고서2020.pdf\n",
      "[49/84] 읽는 중: 우리은행_사업보고서2021.pdf\n",
      "[50/84] 읽는 중: 우리은행_사업보고서2022.pdf\n",
      "[51/84] 읽는 중: 우리은행_사업보고서2023.pdf\n",
      "[52/84] 읽는 중: 우리은행_사업보고서2024.pdf\n",
      "[53/84] 읽는 중: 우리은행_지속가능경영2022.pdf\n",
      "[54/84] 읽는 중: 우리은행_지속가능경영2023.pdf\n",
      "[55/84] 읽는 중: 우리은행_지속가능경영2024.pdf\n",
      "[56/84] 읽는 중: 카카오뱅크_사업보고서2020.pdf\n",
      "[57/84] 읽는 중: 카카오뱅크_사업보고서2021.pdf\n",
      "[58/84] 읽는 중: 카카오뱅크_사업보고서2022.pdf\n",
      "[59/84] 읽는 중: 카카오뱅크_사업보고서2023.pdf\n",
      "[60/84] 읽는 중: 카카오뱅크_사업보고서2024.pdf\n",
      "[61/84] 읽는 중: 카카오뱅크_지속가능경영2021.pdf\n",
      "[62/84] 읽는 중: 카카오뱅크_지속가능경영2022.pdf\n",
      "[63/84] 읽는 중: 카카오뱅크_지속가능경영2023.pdf\n",
      "[64/84] 읽는 중: 카카오뱅크_지속가능경영2024.pdf\n",
      "[65/84] 읽는 중: 케이뱅크_사업보고서2021.pdf\n",
      "[66/84] 읽는 중: 케이뱅크_사업보고서2022.pdf\n",
      "[67/84] 읽는 중: 케이뱅크_사업보고서2023.pdf\n",
      "[68/84] 읽는 중: 케이뱅크_사업보고서2024.pdf\n",
      "[69/84] 읽는 중: 토스뱅크_감사보고서2021.pdf\n",
      "[70/84] 읽는 중: 토스뱅크_감사보고서2022.pdf\n",
      "[71/84] 읽는 중: 토스뱅크_감사보고서2023.pdf\n",
      "[72/84] 읽는 중: 토스뱅크_감사보고서2024.pdf\n",
      "[73/84] 읽는 중: 하나금융_지속가능경영2019.pdf\n",
      "[74/84] 읽는 중: 하나금융_지속가능경영2020.pdf\n",
      "[75/84] 읽는 중: 하나금융_지속가능경영2021.pdf\n",
      "[76/84] 읽는 중: 하나금융_지속가능경영2022.pdf\n",
      "[77/84] 읽는 중: 하나금융_지속가능경영2023.pdf\n",
      "[78/84] 읽는 중: 하나금융_지속가능경영2024.pdf\n",
      "[79/84] 읽는 중: 하나은행_사업보고서2019.pdf\n",
      "[80/84] 읽는 중: 하나은행_사업보고서2020.pdf\n",
      "[81/84] 읽는 중: 하나은행_사업보고서2021.pdf\n",
      "[82/84] 읽는 중: 하나은행_사업보고서2022.pdf\n",
      "[83/84] 읽는 중: 하나은행_사업보고서2023.pdf\n",
      "[84/84] 읽는 중: 하나은행_사업보고서2024.pdf\n",
      "\n",
      "✅ 완료! 결과 저장: C:\\Users\\DS\\Downloads\\ai_credit_extraction_results.csv\n",
      "\n",
      "=== 은행-연도 요약(최대값) ===\n",
      "year   2019  2020  2021  2022  2023  2024\n",
      "bank                                     \n",
      "국민은행      0     0     0     1     0     0\n",
      "기업은행      0     0     0     0     0     1\n",
      "농협은행      0     0     0     0     1     1\n",
      "미확인       1     1     0     0     0     0\n",
      "신한은행      0     0     1     0     0     0\n",
      "우리은행      0     0     0     0     1     1\n",
      "카카오뱅크     0     0     0     0     1     1\n",
      "케이뱅크      0     0     0     0     0     0\n",
      "토스뱅크      0     0     0     0     0     0\n",
      "하나은행      0     0     1     1     0     0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import pdfplumber\n",
    "except ImportError:\n",
    "    raise SystemExit(\"pdfplumber가 없습니다. conda/pip로 설치하세요: pip install pdfplumber\")\n",
    "\n",
    "# =========================\n",
    "# 1) 너 환경에 맞게 수정\n",
    "# =========================\n",
    "ROOT_DIR = r\"C:\\Users\\DS\\Downloads\"   # <- PDF들 있는 폴더(하위폴더 포함 스캔)\n",
    "OUTPUT_CSV = os.path.join(ROOT_DIR, \"ai_credit_extraction_results.csv\")\n",
    "\n",
    "# 은행 9개 (너가 말한 기준)\n",
    "BANKS = [\n",
    "    \"국민은행\", \"신한은행\", \"우리은행\", \"기업은행\", \"하나은행\",\n",
    "    \"케이뱅크\", \"카카오뱅크\", \"토스뱅크\", \"농협은행\"\n",
    "]\n",
    "\n",
    "COM = [\n",
    "    \"농협금융\", \"하나금융\", \"우리금융\"\n",
    "]\n",
    "\n",
    "YEARS = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "# 파일명 패턴(너가 말한 룰 반영)\n",
    "# - 농협/토스: 은행명_감사보고서2019 (기간 2019~2024)\n",
    "# - 그 외: 은행명_사업보고서_연도\n",
    "# - 지속가능: 은행명_지속가능경영_연도\n",
    "PATTERNS = [\n",
    "    \"{bank}_사업보고서{year}*.pdf\",\n",
    "    \"{bank}_지속가능경영{year}*.pdf\",\n",
    "    \"{com}_지속가능경영{year}*.pdf\",\n",
    "    \"{bank}_감사보고서{year}*.pdf\",\n",
    "    \"{bank}_감사보고서{year}*.PDF\",\n",
    "    \"{bank}_사업보고서{year}*.PDF\",\n",
    "    \"{bank}_지속가능경영{year}*.PDF\",\n",
    "    \"{com}_지속가능경영{year}*.PDF\",\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# 2) 탐지 규칙 (A 정의)\n",
    "# =========================\n",
    "# 'AI' 오탐 방지: 단어 경계로 AI만\n",
    "AI_TOKEN = re.compile(r\"\\bAI\\b\", re.IGNORECASE)\n",
    "\n",
    "# 대출심사/신용평가/리스크관리 키워드(한국어+영문 일부)\n",
    "DOMAIN_KEYWORDS = [\n",
    "    r\"대출\", r\"여신\", r\"심사\", r\"신용\", r\"평가\", r\"스코어\", r\"스코어링\",\n",
    "    r\"리스크\", r\"위험\", r\"신용위험\", r\"시장위험\", r\"운영위험\",\n",
    "    r\"FDS\", r\"AML\", r\"이상거래\", r\"부정거래\", r\"사기\", r\"Fraud\",\n",
    "    r\"신용평가모형\", r\"리스크관리\", r\"Risk\"\n",
    "]\n",
    "DOMAIN_RE = re.compile(\"|\".join(DOMAIN_KEYWORDS), re.IGNORECASE)\n",
    "\n",
    "# 운영/상용화 여부를 강하게 시사하는 단어(있으면 더 \"확실\")\n",
    "OPERATION_HINTS = [\n",
    "    r\"운영\", r\"상용\", r\"적용\\s*중\", r\"도입\\s*후\", r\"전사\", r\"확대\\s*적용\",\n",
    "    r\"고도화\", r\"활용\\s*중\", r\"적용\", r\"구축\\s*완료\"\n",
    "]\n",
    "OP_RE = re.compile(\"|\".join(OPERATION_HINTS), re.IGNORECASE)\n",
    "\n",
    "# PoC/시범/개발중이면 0으로 떨어뜨릴 때 사용(근거에 표시)\n",
    "PILOT_HINTS = [\n",
    "    r\"시범\", r\"파일럿\", r\"PoC\", r\"검증\", r\"테스트\", r\"개발\\s*중\", r\"추진\", r\"예정\"\n",
    "]\n",
    "PILOT_RE = re.compile(\"|\".join(PILOT_HINTS), re.IGNORECASE)\n",
    "\n",
    "# 문장 분리\n",
    "SENT_SPLIT = re.compile(r\"(?:[\\.?!。]+|다\\.)\\s*|[\\n\\r]+\")\n",
    "\n",
    "# AI와 도메인 키워드가 가까이 있는지 확인할 윈도우(문장 단위로 이미 좁지만 추가 안전장치)\n",
    "MAX_CHAR_WINDOW = 250  # 같은 문장 내에서 AI 주변 250자 내 도메인키워드 있으면 인정\n",
    "\n",
    "# =========================\n",
    "# 3) PDF 텍스트 추출 함수\n",
    "# =========================\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    texts = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            t = page.extract_text() or \"\"\n",
    "            if t.strip():\n",
    "                texts.append(t)\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "def find_evidence_sentences(text: str):\n",
    "    \"\"\"\n",
    "    반환: (evidence_list)\n",
    "    evidence_list item:\n",
    "      dict(sentence, has_op, has_pilot, ai_pos, domain_pos)\n",
    "    \"\"\"\n",
    "    evidences = []\n",
    "    sentences = [s.strip() for s in SENT_SPLIT.split(text) if s and s.strip()]\n",
    "    for s in sentences:\n",
    "        # AI 토큰 먼저 체크\n",
    "        m_ai = AI_TOKEN.search(s)\n",
    "        if not m_ai:\n",
    "            continue\n",
    "\n",
    "        # 같은 문장에 도메인 키워드가 있는지(혹은 AI 주변 윈도우 내)\n",
    "        # 1) 문장 전체에 도메인 키워드\n",
    "        if DOMAIN_RE.search(s):\n",
    "            ok = True\n",
    "        else:\n",
    "            ok = False\n",
    "\n",
    "        # 2) 추가로 AI 주변 window 검사(문장 내에서만)\n",
    "        if not ok:\n",
    "            ai_i = m_ai.start()\n",
    "            lo = max(0, ai_i - MAX_CHAR_WINDOW)\n",
    "            hi = min(len(s), ai_i + MAX_CHAR_WINDOW)\n",
    "            if DOMAIN_RE.search(s[lo:hi]):\n",
    "                ok = True\n",
    "\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        evidences.append({\n",
    "            \"sentence\": s,\n",
    "            \"has_operation_hint\": bool(OP_RE.search(s)),\n",
    "            \"has_pilot_hint\": bool(PILOT_RE.search(s)),\n",
    "        })\n",
    "    return evidences\n",
    "\n",
    "# =========================\n",
    "# 4) 파일 수집\n",
    "# =========================\n",
    "def collect_files():\n",
    "    files = []\n",
    "    for bank in BANKS:\n",
    "        for year in YEARS:\n",
    "            for com in COM:\n",
    "                for pat in PATTERNS:\n",
    "                    g = os.path.join(ROOT_DIR, \"**\", pat.format(bank=bank, year=year, com=com))\n",
    "                    files.extend(glob.glob(g, recursive=True))\n",
    "    # 중복 제거\n",
    "    files = sorted(list(dict.fromkeys(files)))\n",
    "    return files\n",
    "\n",
    "def infer_bank_year_from_filename(path: str):\n",
    "    base = os.path.basename(path)\n",
    "    bank = next((b for b in BANKS if b in base), None)\n",
    "    year = None\n",
    "    for y in YEARS:\n",
    "        if str(y) in base:\n",
    "            year = y\n",
    "            break\n",
    "    doc_type = None\n",
    "    if \"사업보고서\" in base:\n",
    "        doc_type = \"사업보고서\"\n",
    "    elif \"감사보고서\" in base:\n",
    "        doc_type = \"감사보고서\"\n",
    "    elif \"지속가능경영\" in base:\n",
    "        doc_type = \"지속가능경영\"\n",
    "    else:\n",
    "        doc_type = \"기타\"\n",
    "    return bank, year, doc_type\n",
    "\n",
    "# =========================\n",
    "# 5) 메인 실행\n",
    "# =========================\n",
    "def main():\n",
    "    pdf_files = collect_files()\n",
    "    if not pdf_files:\n",
    "        print(\"❌ 매칭되는 PDF 파일을 못 찾았어. ROOT_DIR/파일명 패턴 확인해줘.\")\n",
    "        print(\"현재 ROOT_DIR:\", ROOT_DIR)\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for i, pdf_path in enumerate(pdf_files, 1):\n",
    "        bank, year, doc_type = infer_bank_year_from_filename(pdf_path)\n",
    "        if bank is None or year is None:\n",
    "            # 파일명이 규칙과 다르면 일단 스킵하지 말고 기록은 남김\n",
    "            bank = bank or \"미확인\"\n",
    "            year = year or \"미확인\"\n",
    "\n",
    "        print(f\"[{i}/{len(pdf_files)}] 읽는 중: {os.path.basename(pdf_path)}\")\n",
    "\n",
    "        try:\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "        except Exception as e:\n",
    "            rows.append({\n",
    "                \"bank\": bank, \"year\": year, \"doc_type\": doc_type,\n",
    "                \"pdf_path\": pdf_path, \"ai_credit_flag\": 0,\n",
    "                \"confidence\": \"error\",\n",
    "                \"evidence\": \"\",\n",
    "                \"note\": f\"PDF 읽기 실패: {e}\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        evidences = find_evidence_sentences(text)\n",
    "\n",
    "        if not evidences:\n",
    "            rows.append({\n",
    "                \"bank\": bank, \"year\": year, \"doc_type\": doc_type,\n",
    "                \"pdf_path\": pdf_path, \"ai_credit_flag\": 0,\n",
    "                \"confidence\": \"none\",\n",
    "                \"evidence\": \"\",\n",
    "                \"note\": \"\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # 판정 로직:\n",
    "        # - 근거 문장 중 operation_hint가 있으면 high\n",
    "        # - pilot_hint만 있고 operation_hint 없으면 low(일단 0으로 두고 note에 표시)\n",
    "        has_op_any = any(ev[\"has_operation_hint\"] for ev in evidences)\n",
    "        has_pilot_any = any(ev[\"has_pilot_hint\"] for ev in evidences)\n",
    "\n",
    "        if has_op_any:\n",
    "            flag = 1\n",
    "            conf = \"high\"\n",
    "            note = \"\"\n",
    "        else:\n",
    "            # 운영근거 없고 파일럿/개발중만 보이면 0 유지(너 규칙)\n",
    "            flag = 0\n",
    "            conf = \"low\"\n",
    "            note = \"AI+도메인 언급은 있으나 운영/상용 근거가 약함(파일럿/추진 가능)\"\n",
    "\n",
    "        # 증거는 최대 3개만 저장(너무 길어지는 것 방지)\n",
    "        evidence_text = \" | \".join([ev[\"sentence\"] for ev in evidences[:3]])\n",
    "\n",
    "        # 파일럿 힌트가 같이 있으면 note 강화\n",
    "        if has_pilot_any and not has_op_any:\n",
    "            note = (note + \"; \" if note else \"\") + \"문장에 시범/파일럿/개발중 힌트 포함\"\n",
    "\n",
    "        rows.append({\n",
    "            \"bank\": bank, \"year\": year, \"doc_type\": doc_type,\n",
    "            \"pdf_path\": pdf_path,\n",
    "            \"ai_credit_flag\": flag,\n",
    "            \"confidence\": conf,\n",
    "            \"evidence\": evidence_text,\n",
    "            \"note\": note\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"\\n✅ 완료! 결과 저장:\", OUTPUT_CSV)\n",
    "\n",
    "    # 은행별/연도별 요약도 같이 보여주기\n",
    "    try:\n",
    "        pivot = df.pivot_table(index=[\"bank\"], columns=[\"year\"], values=\"ai_credit_flag\", aggfunc=\"max\", fill_value=0)\n",
    "        print(\"\\n=== 은행-연도 요약(최대값) ===\")\n",
    "        print(pivot)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d65466-db03-416d-8ead-4d63217cde46",
   "metadata": {
    "id": "94d65466-db03-416d-8ead-4d63217cde46",
    "outputId": "41ec9383-d962-4ab1-8577-cb7e662e6e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ds\\anaconda3\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ds\\anaconda3\\lib\\site-packages (4.13.5)\n",
      "Requirement already satisfied: lxml in c:\\users\\ds\\anaconda3\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ds\\anaconda3\\lib\\site-packages (2.3.3)\n",
      "Collecting trafilatura\n",
      "  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ds\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Collecting dateparser\n",
      "  Downloading dateparser-1.3.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.14.3-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting courlan>=1.3.2 (from trafilatura)\n",
      "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting htmldate>=1.9.2 (from trafilatura)\n",
      "  Downloading htmldate-1.9.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting justext>=3.0.1 (from trafilatura)\n",
      "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: colorama in c:\\users\\ds\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: regex>=2024.9.11 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from dateparser) (2025.9.1)\n",
      "Collecting tzlocal>=0.2 (from dateparser)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: babel>=2.16.0 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
      "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n",
      "  Downloading tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting lxml_html_clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura)\n",
      "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ds\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
      "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Downloading dateparser-1.3.0-py3-none-any.whl (318 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp313-cp313-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 54.6 MB/s  0:00:00\n",
      "Downloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
      "Downloading htmldate-1.9.4-py3-none-any.whl (31 kB)\n",
      "Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
      "   ---------------------------------------- 0.0/837.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 837.9/837.9 kB 29.9 MB/s  0:00:00\n",
      "Downloading tld-0.13.1-py2.py3-none-any.whl (274 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Downloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n",
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6104 sha256=82a2384497d609064e0153968f0f2add865810516c4c2e80f959bd85e71148fd\n",
      "  Stored in directory: c:\\users\\ds\\appdata\\local\\pip\\cache\\wheels\\3d\\4d\\ef\\37cdccc18d6fd7e0dd7817dcdf9146d4d6789c32a227a28134\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, tzlocal, tld, rapidfuzz, lxml_html_clean, feedparser, dateparser, courlan, justext, htmldate, trafilatura\n",
      "\n",
      "   ------- --------------------------------  2/11 [tld]\n",
      "   ---------- -----------------------------  3/11 [rapidfuzz]\n",
      "   ------------------ ---------------------  5/11 [feedparser]\n",
      "   --------------------- ------------------  6/11 [dateparser]\n",
      "   --------------------- ------------------  6/11 [dateparser]\n",
      "   --------------------- ------------------  6/11 [dateparser]\n",
      "   --------------------- ------------------  6/11 [dateparser]\n",
      "   --------------------- ------------------  6/11 [dateparser]\n",
      "   ------------------------- --------------  7/11 [courlan]\n",
      "   -------------------------------- -------  9/11 [htmldate]\n",
      "   ------------------------------------ --- 10/11 [trafilatura]\n",
      "   ---------------------------------------- 11/11 [trafilatura]\n",
      "\n",
      "Successfully installed courlan-1.3.2 dateparser-1.3.0 feedparser-6.0.12 htmldate-1.9.4 justext-3.0.2 lxml_html_clean-0.4.3 rapidfuzz-3.14.3 sgmllib3k-1.0.0 tld-0.13.1 trafilatura-2.0.0 tzlocal-5.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 lxml pandas trafilatura feedparser tqdm dateparser rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e1973-761b-458c-b3c1-0ebc630c6cb0",
   "metadata": {
    "id": "197e1973-761b-458c-b3c1-0ebc630c6cb0",
    "outputId": "2c826a7a-5855-4a6c-ce1d-1905f108573b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[국민은행] official/press crawl: 100%|██████████| 153/153 [03:36<00:00,  1.42s/it]\n",
      "[신한은행] official/press crawl: 0it [00:00, ?it/s]\n",
      "[우리은행] official/press crawl: 100%|██████████| 70/70 [01:29<00:00,  1.27s/it]\n",
      "[기업은행] official/press crawl: 0it [00:00, ?it/s]\n",
      "[하나은행] official/press crawl: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "[농협은행] official/press crawl: 100%|██████████| 7/7 [00:08<00:00,  1.20s/it]\n",
      "[케이뱅크] official/press crawl: 0it [00:00, ?it/s]\n",
      "[토스뱅크] official/press crawl: 100%|██████████| 27/27 [00:35<00:00,  1.32s/it]\n",
      "[카카오뱅크] official/press crawl: 100%|██████████| 13/13 [00:15<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] saved: ai_credit_web_evidence.csv  (rows=7)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import urllib.parse\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import feedparser\n",
    "import dateparser\n",
    "from rapidfuzz import fuzz\n",
    "import trafilatura\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) 확정한 AI_credit 기준\n",
    "# =========================\n",
    "\n",
    "INCLUDE_KEYWORDS = [\n",
    "    \"인공지능 기반 신용평가\",\n",
    "    \"AI 기반 대출심사\",\n",
    "    \"자동심사 시스템\",\n",
    "    \"머신러닝 기반 리스크관리\",\n",
    "    \"FDS 고도화\",\n",
    "    \"AML AI 시스템\",\n",
    "]\n",
    "\n",
    "AI_TOKENS = [\"AI\", \"인공지능\", \"머신러닝\", \"딥러닝\", \"ML\", \"학습모델\"]\n",
    "CREDIT_RISK_TOKENS = [\n",
    "    \"신용평가\", \"신용\", \"대출\", \"여신\", \"심사\", \"스코어링\", \"리스크\", \"위험\", \"리스크관리\",\n",
    "    \"이상거래\", \"FDS\", \"자금세탁\", \"AML\", \"제재\", \"컴플라이언스\", \"사기탐지\"\n",
    "]\n",
    "\n",
    "EXCLUDE_KEYWORDS = [\n",
    "    \"AI 상담\",\n",
    "    \"AI 챗봇\",\n",
    "    \"챗봇\",\n",
    "    \"고객 응대\",\n",
    "    \"디지털 창구\",\n",
    "    \"모바일 상담\",\n",
    "    \"콜센터\",\n",
    "    \"FAQ\",\n",
    "]\n",
    "\n",
    "# 도입연도 정의 = “상용화/운영 중이라고 명시된 최초 연도”\n",
    "OPS_TRIGGERS = [\n",
    "    \"상용화\", \"운영\", \"운영 중\", \"적용 중\", \"도입 후 운영\", \"가동\", \"활용 중\", \"실서비스\", \"본격 운영\"\n",
    "]\n",
    "NON_OPS_TRIGGERS = [\n",
    "    \"PoC\", \"시범\", \"파일럿\", \"검증\", \"개발\", \"구축\", \"예정\", \"추진\", \"계획\", \"준비\"\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) 유틸\n",
    "# =========================\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; AIcreditResearchBot/1.0; +https://example.org/bot)\"\n",
    "}\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "    return s\n",
    "\n",
    "def year_from_text(text: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    본문에서 연도 후보 추출 (예: 2021년, 2021. 등)\n",
    "    여러 개면 '운영 트리거'와 가까운 연도를 우선 선택.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    years = [int(y) for y in re.findall(r\"(19\\d{2}|20\\d{2})\\s*년?\", text)]\n",
    "    years = [y for y in years if 1990 <= y <= 2035]\n",
    "    if not years:\n",
    "        return None\n",
    "    # 일단 최빈/최소 같은 단순규칙 대신, \"운영\" 근처를 우선\n",
    "    # 운영 트리거 등장 위치 기준으로 가까운 연도 선택\n",
    "    ops_positions = []\n",
    "    for trig in OPS_TRIGGERS:\n",
    "        for m in re.finditer(re.escape(trig), text):\n",
    "            ops_positions.append(m.start())\n",
    "    if not ops_positions:\n",
    "        return min(years)\n",
    "    best = None\n",
    "    best_dist = 10**9\n",
    "    for y in set(years):\n",
    "        for m in re.finditer(str(y), text):\n",
    "            for p in ops_positions:\n",
    "                d = abs(m.start() - p)\n",
    "                if d < best_dist:\n",
    "                    best_dist = d\n",
    "                    best = y\n",
    "    return best or min(years)\n",
    "\n",
    "def split_sentences_ko(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    한국어 문장 분리(완벽하진 않지만 근거문장 뽑기엔 충분)\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    if not text:\n",
    "        return []\n",
    "    # 마침표/물음표/느낌표/다/함/됨 등 기반 분리(간단 버전)\n",
    "    chunks = re.split(r\"(?<=[\\.\\?\\!]|다|함|됨)\\s+\", text)\n",
    "    chunks = [c.strip() for c in chunks if len(c.strip()) >= 10]\n",
    "    return chunks\n",
    "\n",
    "def has_exclude(text: str) -> bool:\n",
    "    t = text.lower()\n",
    "    return any(k.lower() in t for k in EXCLUDE_KEYWORDS)\n",
    "\n",
    "def has_ai_credit_signal(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    A 변수: 신용/리스크 AI만\n",
    "    - (AI 토큰 1개 이상) AND (신용/리스크 토큰 1개 이상)\n",
    "    - 또는 너가 지정한 include 문구(정확/유사)\n",
    "    \"\"\"\n",
    "    t = text\n",
    "    # 1) exclude 강하게 걸기\n",
    "    if has_exclude(t):\n",
    "        return False\n",
    "\n",
    "    # 2) include 문구 유사매칭(완전일치 안해도 잡기)\n",
    "    for kw in INCLUDE_KEYWORDS:\n",
    "        if kw in t:\n",
    "            return True\n",
    "        # 유사도(짧은 문구는 과탐 가능하니 80 이상)\n",
    "        if fuzz.partial_ratio(kw, t) >= 85:\n",
    "            return True\n",
    "\n",
    "    # 3) 토큰 조합\n",
    "    has_ai = any(tok in t for tok in AI_TOKENS)\n",
    "    has_domain = any(tok in t for tok in CREDIT_RISK_TOKENS)\n",
    "    return bool(has_ai and has_domain)\n",
    "\n",
    "def is_operational_claim(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    ‘상용화/운영 중’ 트리거가 있으면 True.\n",
    "    단, PoC/파일럿/계획 같은 NON_OPS가 강하면 False 우선.\n",
    "    \"\"\"\n",
    "    t = text\n",
    "    if any(x in t for x in NON_OPS_TRIGGERS):\n",
    "        # 문장에 PoC/파일럿이 함께 있으면 운영으로 보지 않음(너 규칙)\n",
    "        # 단, \"운영 중\"이 명시되면 운영이 이김\n",
    "        if any(x in t for x in OPS_TRIGGERS):\n",
    "            return True\n",
    "        return False\n",
    "    return any(x in t for x in OPS_TRIGGERS)\n",
    "\n",
    "def extract_evidence_sentences(text: str, max_evidences: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    근거문장(최대 N개) 뽑기:\n",
    "    - AI_credit signal 문장\n",
    "    - 그중 운영 트리거 있는 문장을 최우선\n",
    "    \"\"\"\n",
    "    sents = split_sentences_ko(text)\n",
    "    candidates = [s for s in sents if has_ai_credit_signal(s)]\n",
    "    if not candidates:\n",
    "        return []\n",
    "    ops = [s for s in candidates if is_operational_claim(s)]\n",
    "    ordered = ops + [s for s in candidates if s not in ops]\n",
    "    return ordered[:max_evidences]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) 본문 추출\n",
    "# =========================\n",
    "\n",
    "def fetch_url(url: str, timeout: int = 20) -> Optional[str]:\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_main_text(url: str, html: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    trafilatura로 메인 텍스트 추출\n",
    "    \"\"\"\n",
    "    if html is None:\n",
    "        html = fetch_url(url)\n",
    "    if not html:\n",
    "        return \"\"\n",
    "    downloaded = trafilatura.extract(html, url=url, include_comments=False, include_tables=False)\n",
    "    return clean_text(downloaded or \"\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) 공식 사이트/보도자료 URL 수집\n",
    "#   - sitemap.xml 우선\n",
    "#   - 안 되면 seed 목록 페이지에서 링크 수집\n",
    "# =========================\n",
    "\n",
    "def parse_sitemap_urls(sitemap_url: str, limit: int = 5000) -> List[str]:\n",
    "    xml = fetch_url(sitemap_url)\n",
    "    if not xml:\n",
    "        return []\n",
    "    soup = BeautifulSoup(xml, \"xml\")\n",
    "    locs = [loc.get_text(strip=True) for loc in soup.find_all(\"loc\")]\n",
    "    return locs[:limit]\n",
    "\n",
    "def collect_links_from_listing(listing_url: str, domain_filter: Optional[str] = None, limit: int = 300) -> List[str]:\n",
    "    html = fetch_url(listing_url)\n",
    "    if not html:\n",
    "        return []\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    links = []\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        full = urllib.parse.urljoin(listing_url, href)\n",
    "        if domain_filter and urllib.parse.urlparse(full).netloc and domain_filter not in full:\n",
    "            continue\n",
    "        links.append(full)\n",
    "    # 중복 제거\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for u in links:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            uniq.append(u)\n",
    "    return uniq[:limit]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) 뉴스 수집 (권장: Google News RSS)\n",
    "# =========================\n",
    "\n",
    "def google_news_rss(query: str, hl: str = \"ko\", gl: str = \"KR\", ceid: str = \"KR:ko\") -> str:\n",
    "    q = urllib.parse.quote(query)\n",
    "    return f\"https://news.google.com/rss/search?q={q}&hl={hl}&gl={gl}&ceid={ceid}\"\n",
    "\n",
    "def collect_news_urls_google_rss(query: str, max_items: int = 50) -> List[Tuple[str, Optional[str]]]:\n",
    "    \"\"\"\n",
    "    return: [(url, published_str)]\n",
    "    \"\"\"\n",
    "    rss_url = google_news_rss(query)\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    out = []\n",
    "    for e in feed.entries[:max_items]:\n",
    "        link = e.get(\"link\")\n",
    "        published = e.get(\"published\") or e.get(\"updated\")\n",
    "        if link:\n",
    "            out.append((link, published))\n",
    "    return out\n",
    "\n",
    "# (옵션) GDELT 2.1: 훨씬 많이 잡히는데, 한국어 커버리지 편차 있음\n",
    "def collect_news_urls_gdelt(query: str, max_records: int = 100) -> List[str]:\n",
    "    base = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"mode\": \"ArtList\",\n",
    "        \"format\": \"json\",\n",
    "        \"maxrecords\": str(max_records),\n",
    "        \"sort\": \"HybridRel\"\n",
    "    }\n",
    "    url = base + \"?\" + urllib.parse.urlencode(params)\n",
    "    js = None\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=20)\n",
    "        if r.status_code != 200:\n",
    "            return []\n",
    "        js = r.json()\n",
    "    except Exception:\n",
    "        return []\n",
    "    arts = js.get(\"articles\", []) if isinstance(js, dict) else []\n",
    "    return [a.get(\"url\") for a in arts if a.get(\"url\")]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) 결과 스키마\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class EvidenceRow:\n",
    "    bank: str\n",
    "    source_type: str     # \"official\" / \"press\" / \"news\"\n",
    "    url: str\n",
    "    title: str\n",
    "    published: str\n",
    "    year_operational_first: Optional[int]\n",
    "    ai_credit_mention: int     # 1/0\n",
    "    operational_claim: int     # 1/0\n",
    "    evidence_1: str\n",
    "    evidence_2: str\n",
    "    evidence_3: str\n",
    "    notes: str\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) 파이프라인\n",
    "# =========================\n",
    "\n",
    "def analyze_url(bank: str, url: str, source_type: str, published: Optional[str] = None) -> EvidenceRow:\n",
    "    html = fetch_url(url)\n",
    "    title = \"\"\n",
    "    if html:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        if soup.title and soup.title.get_text():\n",
    "            title = clean_text(soup.title.get_text())\n",
    "    text = extract_main_text(url, html=html)\n",
    "\n",
    "    evidences = extract_evidence_sentences(text, max_evidences=3)\n",
    "    ai_mention = 1 if len(evidences) > 0 else 0\n",
    "    ops_claim = 1 if any(is_operational_claim(e) for e in evidences) else 0\n",
    "\n",
    "    # 운영/상용화 최초 연도: 운영 트리거 문장 중심으로 추정\n",
    "    year = None\n",
    "    if ops_claim:\n",
    "        year = year_from_text(\" \".join([e for e in evidences if is_operational_claim(e)])) or year_from_text(text)\n",
    "    else:\n",
    "        year = None\n",
    "\n",
    "    # published 파싱 (RSS 등에서 받은 값이 있으면 사용)\n",
    "    pub_str = clean_text(published or \"\")\n",
    "    if pub_str:\n",
    "        dt = dateparser.parse(pub_str, languages=[\"ko\", \"en\"])\n",
    "        if dt:\n",
    "            pub_str = dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return EvidenceRow(\n",
    "        bank=bank,\n",
    "        source_type=source_type,\n",
    "        url=url,\n",
    "        title=title,\n",
    "        published=pub_str,\n",
    "        year_operational_first=year,\n",
    "        ai_credit_mention=ai_mention,\n",
    "        operational_claim=ops_claim,\n",
    "        evidence_1=evidences[0] if len(evidences) > 0 else \"\",\n",
    "        evidence_2=evidences[1] if len(evidences) > 1 else \"\",\n",
    "        evidence_3=evidences[2] if len(evidences) > 2 else \"\",\n",
    "        notes=\"\"\n",
    "    )\n",
    "\n",
    "def run_official_press_crawl(bank: str, seeds: Dict[str, List[str]], sleep_sec: float = 0.8, max_urls: int = 300) -> List[EvidenceRow]:\n",
    "    \"\"\"\n",
    "    seeds 예시:\n",
    "    {\n",
    "      \"sitemaps\": [\"https://.../sitemap.xml\", ...],\n",
    "      \"listings\": [\"https://.../press\", \"https://.../news\", ...],\n",
    "      \"domain\": \"example.com\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    domain = seeds.get(\"domain\", \"\")\n",
    "    urls = []\n",
    "\n",
    "    # 1) sitemap 기반 수집\n",
    "    for sm in seeds.get(\"sitemaps\", []):\n",
    "        urls.extend(parse_sitemap_urls(sm, limit=max_urls))\n",
    "\n",
    "    # 2) listing 기반 수집(공지/보도자료)\n",
    "    for lst in seeds.get(\"listings\", []):\n",
    "        urls.extend(collect_links_from_listing(lst, domain_filter=domain, limit=max_urls))\n",
    "\n",
    "    # 정리 + 도메인 필터 + 중복제거\n",
    "    filtered = []\n",
    "    seen = set()\n",
    "    for u in urls:\n",
    "        if domain and domain not in u:\n",
    "            continue\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            filtered.append(u)\n",
    "\n",
    "    filtered = filtered[:max_urls]\n",
    "\n",
    "    rows = []\n",
    "    for u in tqdm(filtered, desc=f\"[{bank}] official/press crawl\"):\n",
    "        row = analyze_url(bank, u, source_type=\"official\")\n",
    "        if row.ai_credit_mention == 1:\n",
    "            # 신호 있는 것만 저장(노이즈 줄이기)\n",
    "            rows.append(row)\n",
    "        time.sleep(sleep_sec)\n",
    "    return rows\n",
    "\n",
    "def run_news_crawl(bank: str, sleep_sec: float = 0.8, per_query: int = 30) -> List[EvidenceRow]:\n",
    "    \"\"\"\n",
    "    뉴스는 키워드 쿼리로 잡고, 본문에서 AI_credit만 남김.\n",
    "    \"\"\"\n",
    "    queries = [\n",
    "        f\"{bank} 인공지능 신용평가 상용화\",\n",
    "        f\"{bank} AI 대출심사 운영\",\n",
    "        f\"{bank} 머신러닝 리스크관리 운영\",\n",
    "        f\"{bank} FDS 고도화 AI 운영\",\n",
    "        f\"{bank} AML AI 시스템 운영\",\n",
    "    ]\n",
    "    seen = set()\n",
    "    rows = []\n",
    "\n",
    "    for q in queries:\n",
    "        items = collect_news_urls_google_rss(q, max_items=per_query)\n",
    "        for (url, published) in items:\n",
    "            if url in seen:\n",
    "                continue\n",
    "            seen.add(url)\n",
    "            row = analyze_url(bank, url, source_type=\"news\", published=published)\n",
    "            if row.ai_credit_mention == 1:\n",
    "                rows.append(row)\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) 은행별 seed 템플릿\n",
    "# =========================\n",
    "\n",
    "BANK_SEEDS = {\n",
    "    \"국민은행\": {\n",
    "        \"domain\": \"kbstar.com\",\n",
    "        \"sitemaps\": [],\n",
    "        \"listings\": [\"https://omoney.kbstar.com/quics?page=C017648\"]\n",
    "    },\n",
    "    \"신한은행\": {\n",
    "        \"domain\": \"shinhan.com\",\n",
    "        \"sitemaps\": [],\n",
    "        \"listings\": [\"https://www.shinhan.com/kr/press/pressList.html\"]\n",
    "    },\n",
    "    \"우리은행\": {\n",
    "        \"domain\": \"wooribank.com\",\n",
    "        \"sitemaps\": [],\n",
    "        \"listings\": [\"https://spot.wooribank.com/pot/Dream?withyou=BPPBC0036\"]\n",
    "    },\n",
    "    \"기업은행\": {\n",
    "        \"domain\": \"ibk.co.kr\",\n",
    "        \"sitemaps\": [],\n",
    "        \"listings\": [\"https://www.ibk.co.kr/IBK/notification/pressReleaseList.do\"]\n",
    "    },\n",
    "    \"하나은행\": {\n",
    "        \"domain\": \"kebhana.com\",\n",
    "        \"sitemaps\": [],\n",
    "        \"listings\": [\"https://www.kebhana.com/cont/mall/mall15/index.jsp?MIDX=20959\"]\n",
    "    },\n",
    "    \"농협은행\": {\n",
    "        \"domain\": \"nhbank.com\",\n",
    "        \"sitemaps\": [],\n",
    "        \"listings\": [\"https://www.nhbank.com/nhbank/introduce/press/pressList.do\"]\n",
    "    },\n",
    "    \"케이뱅크\": {\n",
    "        \"domain\": \"kbanknow.com\",\n",
    "        \"sitemaps\": [],\n",
    "        \"listings\": [\"https://kbanknow.com/notice/list.do?menuId=090000\"]\n",
    "    },\n",
    "    \"토스뱅크\": {\n",
    "        \"domain\": \"tossbank.com\",\n",
    "        \"sitemaps\": [],\n",
    "        \"listings\": [\"https://tossbank.com/notice\"]\n",
    "    },\n",
    "    \"카카오뱅크\": {\n",
    "        \"domain\": \"kakaobank.com\",\n",
    "        \"sitemaps\": [],\n",
    "        \"listings\": [\"https://www.kakaobank.com/Corp/News/PressRelease\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def main(\n",
    "    banks: Optional[List[str]] = None,\n",
    "    out_csv: str = \"ai_credit_web_evidence.csv\"\n",
    "):\n",
    "    if banks is None:\n",
    "        banks = list(BANK_SEEDS.keys())\n",
    "\n",
    "    all_rows: List[EvidenceRow] = []\n",
    "\n",
    "    for bank in banks:\n",
    "        seeds = BANK_SEEDS.get(bank, {})\n",
    "        # 1) 공식/보도자료\n",
    "        if seeds:\n",
    "            all_rows.extend(run_official_press_crawl(bank, seeds, sleep_sec=0.8, max_urls=250))\n",
    "\n",
    "        # 2) 뉴스\n",
    "        all_rows.extend(run_news_crawl(bank, sleep_sec=0.8, per_query=25))\n",
    "\n",
    "    # dataframe\n",
    "    df = pd.DataFrame([r.__dict__ for r in all_rows])\n",
    "\n",
    "    # 동일 URL 중복 제거\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "\n",
    "        # 운영 주장(operational_claim=1) 우선 정렬\n",
    "        df = df.sort_values(by=[\"bank\", \"operational_claim\", \"year_operational_first\"], ascending=[True, False, True])\n",
    "\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[DONE] saved: {out_csv}  (rows={len(df)})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3] *",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
